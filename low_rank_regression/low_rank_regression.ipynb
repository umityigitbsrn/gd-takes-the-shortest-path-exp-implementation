{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import normalized_distance_weight_based, normalized_misfit_function_based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def kernel_function(data, weight):\n",
    "    result_vector = np.zeros((data.shape[-1]))\n",
    "    for idx in range(data.shape[-1]):\n",
    "        data_i = data[:, :, idx]\n",
    "        result_vector[idx] = np.matmul(weight.transpose(), np.matmul(data_i, weight)).trace()\n",
    "    return result_vector\n",
    "\n",
    "def loss_function(labels, data, weight):\n",
    "    return 0.5*(np.linalg.norm(kernel_function(data, weight) - labels)**2)\n",
    "\n",
    "def jacobian_kernel(data, weight):\n",
    "    resulting_matrix = []\n",
    "    for idx in range(data.shape[-1]):\n",
    "        data_i = data[:, :, idx]\n",
    "        resulting_matrix.append(np.matmul(data_i, weight).flatten('F'))\n",
    "\n",
    "    return np.vstack(resulting_matrix)\n",
    "\n",
    "def gradient_loss_function(labels, data, weight):\n",
    "    return np.matmul(jacobian_kernel(data, weight).transpose(), kernel_function(data, weight) - labels).reshape(weight.shape, order='F')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def weight_interval(labels, r_dimension, number_of_samples):\n",
    "    min_range = np.sqrt(np.linalg.norm(labels)) / np.power(r_dimension * number_of_samples, 1 / 4)\n",
    "    return min_range, 2*min_range\n",
    "\n",
    "def init_weight(labels, r_dimension, d_dimension, number_of_samples):\n",
    "    min_range, max_range = weight_interval(labels, r_dimension, number_of_samples)\n",
    "    singular = np.diag(np.random.uniform(low=min_range, high=max_range, size=r_dimension))\n",
    "    return np.concatenate([singular, np.zeros((d_dimension - r_dimension, r_dimension))], axis=0)\n",
    "\n",
    "def generate_data(number_of_samples, d_dimension):\n",
    "    data_arr = [np.random.normal(size=(d_dimension, d_dimension)) for _ in range(number_of_samples)]\n",
    "    return np.stack(data_arr, axis=-1)\n",
    "\n",
    "def generate_labels(number_of_samples):\n",
    "    return np.random.choice([-1, 1], number_of_samples)\n",
    "\n",
    "def generate_lr(labels, r_dimension, d_dimension, number_of_samples):\n",
    "    c = number_of_samples / (r_dimension * d_dimension)\n",
    "    lr_constant = (c*math.sqrt(number_of_samples)) / ((r_dimension ** 2)*d_dimension)\n",
    "    return lr_constant / np.linalg.norm(labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d = 100\n",
    "r = 4\n",
    "n = [25, 50, 100, 200]\n",
    "num_of_iterations = 200\n",
    "\n",
    "result_arr = []\n",
    "\n",
    "for num_of_samples_idx in range(len(n)):\n",
    "    data_X = generate_data(n[num_of_samples_idx], d)\n",
    "    label_y = generate_labels(n[num_of_samples_idx])\n",
    "    weight_theta = init_weight(label_y, r, d, n[num_of_samples_idx])\n",
    "    init_weight_theta = copy.deepcopy(weight_theta)\n",
    "\n",
    "    lr = generate_lr(label_y, r, d, n[num_of_samples_idx])\n",
    "\n",
    "    misfit_distance_dict = {\n",
    "        'name': 'n = dr/{}'.format(d*r / n[num_of_samples_idx]),\n",
    "        'arr': []\n",
    "    }\n",
    "\n",
    "    for iter_idx in range(num_of_iterations):\n",
    "        misfit_distance_dict['arr'].append({\n",
    "            'misfit': normalized_misfit_function_based(label_y, kernel_function, data_X, init_weight_theta, weight_theta),\n",
    "            'distance': normalized_distance_weight_based(init_weight_theta, weight_theta)\n",
    "        })\n",
    "        loss = loss_function(label_y, data_X, weight_theta)\n",
    "        weight_theta = weight_theta - lr*gradient_loss_function(label_y, data_X, weight_theta)\n",
    "\n",
    "    result_arr.append(misfit_distance_dict)\n",
    "    with open('./misfit_distance_json/misfit_distance_{}.json'.format(n[num_of_samples_idx]), 'w') as fp:\n",
    "        json.dump(misfit_distance_dict, fp, indent=6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "for elem in result_arr:\n",
    "    x_axis = [iter_dict['distance'] for iter_dict in elem['arr']]\n",
    "    y_axis = [iter_dict['misfit'] for iter_dict in elem['arr']]\n",
    "    plt.plot(x_axis, y_axis, label=elem['name'])\n",
    "plt.xlabel('normalized distance')\n",
    "plt.ylabel('normalized misfit')\n",
    "plt.title('low-rank regression')\n",
    "plt.legend()\n",
    "plt.savefig('low_rank_regression.jpeg', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
